# Лабораторная работа №4
## Деревья решений. Бэггинг и случайный лес

## Цель работы
Изучить принципы работы решающих деревьев, методов бэггинга и случайного леса, а также оценить их качество на задачах классификации и регрессии.

## Часть 1. Основы построения решающего дерева

### Задание 1.1. Расчёт энтропии
**Дано:**
- Всего объектов: 10
- Класс k1: 8
- Класс k2: 2

**Результат:** энтропия = **0.50**

---

### Задание 1.2. Индекс Джини
**Дано:**
- Всего объектов: 12
- Класс k1: 9
- Класс k2: 3

**Результат:** индекс Джини = **0.32**

---

### Задание 1.3. Предсказание в листе (регрессия)
Значения целевой переменной:

```
[1, 10, 5, 18, 100, 30, 50, 61, 84, 47]
```

**Результат:** среднее значение = **40.6**

---

## Часть 2. Решающие деревья

### Задание 2.1. Поиск лучшего разбиения
Для датасета California Housing был выполнен поиск оптимального признака и порога для первого разбиения.

**Лучший признак:** `MedInc`  \
**Лучший порог:** 5.0351  \
**Максимальный выигрыш:** 0.412751

---

### Задание 2.2. Предсказание и важность признаков

- Размер выборки: 100 объектов
- Количество признаков: 3
- Задача: бинарная классификация

**Accuracy:** 0.99

**Важность признаков:**
1. Признак 2 — 88.3%
2. Признак 1 — 7.6%
3. Признак 0 — 4.1%

---

### Задание 2.3. Анализ данных Student Performance

- Количество наблюдений: 258
- Целевая переменная: UNS

**Распределение классов:**
- Низкая успеваемость: 41.5%
- Высокая успеваемость: 58.5%

Наиболее значимый признак: **PEG** (результаты экзамена).

---

### Задание 2.4. Датасет Mushrooms

- Размер датасета: 8124 × 23
- Количество признаков: 22

**Accuracy модели:** 1.000

---

## Часть 3. Бэггинг и случайный лес

### Задание 3.1. Разделение данных Pima Indians Diabetes

- Всего объектов: 768
- Признаков: 8

Данные разделены на обучающую, валидационную и тестовую выборки.

---

### Задание 3.2. Подбор гиперпараметров Decision Tree

**Лучшие параметры:**
```
max_depth = 5
min_samples_leaf = 20
```

**Лучший F1-score:** 0.625

---

### Задание 3.3. BaggingClassifier

- Количество деревьев: 50

**Метрики качества:**
- Accuracy: 0.90
- Precision: 0.78
- Recall: 1.00
- AUC-ROC: 1.00

---

### Задание 3.4. Random Forest

**Лучшие параметры:**
```
max_depth = 3
min_samples_split = 2
```

**AUC-ROC:** 1.00

---

### Задание 3.5. Влияние количества деревьев

Оптимальный диапазон количества деревьев: **20–75**.

---

### Задание 3.6. Важность признаков

Наиболее важные признаки:
1. Glucose
2. BMI
3. Age

---

## Выводы
1. Решающие деревья просты и интерпретируемы
2. Бэггинг снижает переобучение
3. Случайный лес показывает наилучшее качество
4. Анализ важности признаков повышает интерпретируемость моделей

